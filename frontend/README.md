## Privacy
- [Informal Laws](https://www.ucl.ac.uk/data-protection/sites/data-protection/files/using-twitter-research-v1.0.pdf)
- [Twitter Developer Terms of Agreement and Policy](https://developer.twitter.com/en/developer-terms/agreement-and-policy)

They basically say that a user's tweets are free to be used as long as the user is verified and the tweet isn't protected.

## How to Run
**Worked with Python v3.8.5.**

1. Install necessary dependencies.
    ```
    python -m venv twitter_env
    source twitter_env/bin/activate
    pip install -r requirements.txt
    ```

2. For your convenience, get the already-generated data from [box.com](https://ucla.box.com/s/w2bm8kmkvdcnu466iq99gxsq8oesdad3) and place it in the `api` directory.

    If you'd rather generate the data from scratch, write
    `GenerationThread().start()` at the bottom of `app.py` (in the project's root directory) and start the server with steps 3 and 4. Be warned that the generation task takes a while. The main bottleneck is generating polarity scores for each tweet with [Patricia's](https://github.com/PatriciaXiao/TIMME) model (more details in TODO), which takes a few days of leaving my laptop open and the process running. (More details on the generation and rehydration process in TODO.)

3. Add the necessary Twitter Developer API keys to a `.env` file in the project's root directory. Details are on [Twitter's](https://developer.twitter.com/en/support/twitter-api/developer-account) page. These are necessary for data rehydration (pinging the Twitter API to see which tweets are no longer public, for privacy reasons).

    Your `.env` file should look like:
    ```
    BEARER_TOKEN="<YOUR_BEARER_TOKEN>"
    OAUTH_BEARER_TOKEN="<YOUR_OAUTH_BEARER_TOKEN>"
    ```

    If you don't want to wait for your Twitter Developer account to be approved and aren't deploying the project with data rehydration enabled, you can replace `<YOUR_BEARER_TOKEN>` and `<YOUR_OAUTH_BEARER_TOKEN>` with garbage.

4. To start the server, stay in the project's root directory and run:
    ```
    flask run
    ```

5. In another terminal, install the frontend libraries.
    ```
    cd frontend
    yarn
    ```

6. Start the frontend.
    ```
    yarn start
    ```

## Files and their Functionality
Note: The frontend was generated by [Create React App](https://create-react-app.dev/) so there's some boilerplate code lying around.

### `app.py`
 * 1. Interfaces with Patricia's model to generate polarity scores for tweets
 *
 * 2. Delegates all requests from the frontend to the `/flask` endpoint. That's handled in `api/Server.py`.
 *
 * 3. Where lines can be uncommented or added to enable data rehydration/generation.
 *    - Data rehydration simply goes through each tweet in the existing dataset `api/final.csv` and asks the Twitter API whether these tweets are still public.
 *    - The data generation process:
 *      1. Group together the users we're interested in (specified in `api/data_generation/users.txt`) to fit within 1000 characters (the max query length accepted by the Twitter API).
 *
 *      2. For each of those groups, go through the different permutations of relationships. Here, we only consider `mentions` and `retweets_from` so there's only 4 permutations, including their negations. I ignore the `not mentions and not retweet` relationship because that'd yield too many results.
 *
 *      3. Run each result through `process_entry` (in `api/data_generation/data_utils.py`) to massage the results into a format we want.
 *
 *      4. Upon hitting Twitter API's rate limit timeout, save the results to a temporary csv (pathname specified in `api/data_generation/data_constants.py`). So results will be saved in batches to the csv. While waiting for the timeout to end, generate the polarity score for each tweet in the csv with Patricia's trained model. These polarity scores are also generated and saved in batches back to the csv.
 *
 *      5. After getting all polarity scores and tweets, rename the temporary csv to the csv that's actually used by the server (pathname specified in `api/data_generation/data_constants.py`).
 *

### `api/Server.py`
Defines how server should handle GET and POST requests directed to the `/flask` endpoint from the frontend.

Supported operations:
  GET: serves data from `final.csv`
    - usernames
    - num_posts_over_time
    - num_left_right_posts
    - attn_weights
    - post_polarity // for real tweets

  POST:
    - post_polarity // for fake, user-supplied tweets

## Final Report
