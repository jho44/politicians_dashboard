
<!DOCTYPE html>
<html>
<head>
<link rel="stylesheet" href="styles.css">
</head>
<body>
  <div>
    <h2>Description of <span class="code">app.py</span></h2>
    <ol>
      <li>
        Interfaces with Patricia's model to generate polarity scores for tweets
      </li>
      <li>
        Delegates all requests from the frontend to the <span class="code">/flask</span> endpoint. That's handled in <span class="code">api/Server.py</span>.
      </li>
      <li>
        Where lines can be uncommented or added to enable data rehydration/generation.
        <ul>
          <li>
            Data rehydration simply goes through each tweet in the existing dataset <span class="code">api/final.csv</span> and asks the Twitter API whether these tweets are still public.
          </li>
          <li>
            The data generation process:
            <ol>
              <li>
                Group together the users we're interested in (specified in <span class="code">api/data_generation/dict.csv</span>) to fit within 1000 characters (the max query length accepted by the Twitter API).
              </li>
              <li>
                For each of those groups, go through the different permutations of relationships. Here, we only consider <span class="code">mentions</span> and <span class="code">retweets_from</span> so there's only 4 permutations, including their negations. I ignore the <span class="code">not mentions and not retweet</span> relationship because that'd yield too many results.
              </li>
              <li>
                Run each result through <span class="code">process_entry</span> (in <span class="code">api/data_generation/data_utils.py</span>) to massage the results into a format we want.
              </li>
              <li>
                Upon hitting Twitter API's rate limit timeout, save the results to a temporary csv (pathname specified in <span class="code">api/data_generation/data_constants.py</span>). So results will be saved in batches to the csv. While waiting for the timeout to end, generate the polarity score for each tweet in the csv with Patricia's trained model. These polarity scores are also generated and saved in batches back to the csv.
              </li>
              <li>
                After getting all polarity scores and tweets, rename the temporary csv to the csv that's actually used by the server (pathname specified in <span class="code">api/data_generation/data_constants.py</span>).
              </li>
            </ol>
          </li>
        </ul>
      </li>
    </ol>

    <a href="./Server.html">Docs</a> for <span class="code">Server.py</span> generated by <span class="code"><a href="https://docs.python.org/3/library/pydoc.html">pydoc</a></span>.
  </div>
</body>